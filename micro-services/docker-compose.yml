version: "3.9"

services:
  # 1️⃣ MLflow Tracking + Registry
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.11.1
    command: >
      mlflow server --host 0.0.0.0 --port 5000
                   --backend-store-uri sqlite:///mlflow.db
                   --default-artifact-root /mlruns
    volumes:
      - ./mlruns:/mlruns          # shared FileStore for artefacts
    ports: ["5000:5000"]

  # 2️⃣ Model-as-a-Service classifier
  classifier:
    image: ghcr.io/mlflow/mlflow:v2.11.1
    command: >
      mlflow models serve -m models:/msg_cls@champion
                         --env-manager=local          # no Conda
                         --host 0.0.0.0 -p 5001
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    volumes:
      - ./mlruns:/mlruns          # reads artefacts from same volume
    depends_on: [mlflow]
    ports: ["5001:5001"]

  # 3️⃣ Lightweight LLM endpoint (Ollama example)
  llm_service:
    image: ollama/ollama:latest   # listens on 11434/tcp by default
    ports: ["11434:11434"]

  # 4️⃣ FastAPI orchestrator
  api:
    build: ./fastapi_service
    environment:
      - CLS_ENDPOINT=http://classifier:5001/invocations
      - LLM_ENDPOINT=http://llm_service:11434/v1/chat/completions
    depends_on: [classifier, llm_service]
    ports: ["8000:8000"]
